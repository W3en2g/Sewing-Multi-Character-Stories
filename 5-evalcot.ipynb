{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 13:37:55 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 11.9MB/s]                    \n",
      "2024-06-16 13:37:55 INFO: Downloaded file to /home/hansirui/stanza_resources/resources.json\n",
      "2024-06-16 13:37:55 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-06-16 13:37:56 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| coref     | ontonotes_electra-large   |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2024-06-16 13:37:56 INFO: Using device: cuda\n",
      "2024-06-16 13:37:56 INFO: Loading: tokenize\n",
      "2024-06-16 13:37:56 INFO: Loading: mwt\n",
      "2024-06-16 13:37:56 INFO: Loading: coref\n",
      "2024-06-16 13:37:59 INFO: Loading: ner\n",
      "2024-06-16 13:37:59 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "with open('character_table.json', 'r', encoding='utf-8') as file:\n",
    "    character_table = json.load(file)\n",
    "\n",
    "\n",
    "import stanza\n",
    "pipe = stanza.Pipeline(\"en\", processors=\"tokenize,coref,ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_Cooccurrence_count(text,Amcs,Bmcs,Cmcs):\n",
    "    output = pipe(text)\n",
    "    output_json = json.loads(str(output))\n",
    "    count = 0\n",
    "    for sentence in output_json:\n",
    "        for mc in Amcs:\n",
    "            for token in sentence:\n",
    "                try:\n",
    "                    mention_inA = mc in token[\"text\"] or (token[\"coref_chains\"] and mc in token[\"coref_chains\"][0][\"representative_text\"])\n",
    "                except:\n",
    "                    continue\n",
    "                if mention_inA:\n",
    "                    break\n",
    "\n",
    "        for mc in Bmcs:\n",
    "            for token in sentence:\n",
    "                try:\n",
    "                    mention_inB = mc in token[\"text\"] or (token[\"coref_chains\"] and mc in token[\"coref_chains\"][0][\"representative_text\"])\n",
    "                except:\n",
    "                    continue\n",
    "                if mention_inB:\n",
    "                    break\n",
    "        if Cmcs:\n",
    "            for mc in Cmcs:\n",
    "                for token in sentence:\n",
    "                    try:\n",
    "                        mention_inC = mc in token[\"text\"] or (token[\"coref_chains\"] and mc in token[\"coref_chains\"][0][\"representative_text\"])\n",
    "                    except:\n",
    "                        continue\n",
    "                    if mention_inC:\n",
    "                        break\n",
    "        else:\n",
    "            mention_inC = False\n",
    "                  \n",
    "        if (mention_inA and mention_inB) or (mention_inA and mention_inC) or (mention_inB and mention_inC):\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "def get_Sec_metric(results):\n",
    "    count_list = []\n",
    "    for i in trange(len(results)):\n",
    "        aid = results[i]['aid']\n",
    "        bid = results[i]['bid']\n",
    "        cid = results[i]['cid']\n",
    "        outline = results[i]['outline']\n",
    "        response = results[i]['response']\n",
    "\n",
    "        Amcs = character_table[aid]\n",
    "        Bmcs = character_table[bid]\n",
    "\n",
    "        if cid:\n",
    "            Cmcs = character_table[cid]\n",
    "        else:\n",
    "            Cmcs = []\n",
    "        if aid == \"018cc08a-3902-4ebe-9da9-fac862431f96\" and  bid == \"bb669de4-490b-48d8-9811-f81ce446f67a\":\n",
    "            count = 0\n",
    "        else:\n",
    "            try:\n",
    "                count = get_Cooccurrence_count(response,Amcs,Bmcs,Cmcs)\n",
    "            except:\n",
    "                print(outline)\n",
    "                print(response)\n",
    "                raise ValueError\n",
    "            count_list.append(count)\n",
    "    return count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_sentences(tokens_list):\n",
    "    \"\"\"\n",
    "    将Stanza tokenize后的tokens列表转换回句子列表。\n",
    "    :param tokens_list: Stanza tokenize处理后的输出列表\n",
    "    :return: 由句子组成的列表，每个句子为一个字符串\n",
    "    \"\"\"\n",
    "    sentences = []  # 存储还原的句子\n",
    "    current_sentence = \"\"  # 当前正在构建的句子\n",
    "\n",
    "    for token in tokens_list:\n",
    "        # 添加当前token的文本到当前句子，但先不加空格\n",
    "        if token[\"text\"].strip():  # 避免添加空字符串\n",
    "            if current_sentence:  # 如果当前句子非空\n",
    "                # 检查上一个token是否以标点结束，如果是，则不添加额外空格\n",
    "                if not re.match(r'[\\.\\?!]+$', token[\"text\"]) and \\\n",
    "                   not re.match(r'^[\\[\\(\\{\\<\\]\\)\\}\\>]', token[\"text\"]):\n",
    "                    current_sentence += \" \"  # 否则，在新token前添加空格\n",
    "            current_sentence += token[\"text\"]\n",
    "\n",
    "    # 如果current_sentence非空，说明最后一个句子未被加入到sentences中，应将其添加\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def get_mentioned_flag(targeted_sentences,reference_sentence):\n",
    "    targeted_sentences.append(reference_sentence)\n",
    "    embeddings = model.encode(targeted_sentences)\n",
    "    # print(targeted_sentences[-1])\n",
    "    for i in range(len(targeted_sentences)):\n",
    "        similarity_score = 1 - cosine(embeddings[i], embeddings[-1])\n",
    "        # print(targeted_sentences[i])\n",
    "        if similarity_score > 0.5:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# types =[\"a0 a1 B3 a3 a4\", \"a0 B0 a2 B4 a4\",\"a0 B0 C0 a2 B4 C4 a4\",\"a0 B0 a1 B1 a2 B2 a3 B3 a4 B4\"]\n",
    "def get_plot_mentioned(outline,response,type):\n",
    "    try:\n",
    "        output = pipe(response)\n",
    "    except:\n",
    "        print(outline)\n",
    "        print(response)\n",
    "        return False\n",
    "        # raise ValueError(\"error\")\n",
    "    output_json = json.loads(str(output))\n",
    "    targeted_sentences =[]\n",
    "    for sentence in output_json:\n",
    "        bfsent = tokens_to_sentences(sentence)\n",
    "        targeted_sentences.extend(bfsent)\n",
    "\n",
    "    flag = True\n",
    "    for outli in outline:\n",
    "        flag = get_mentioned_flag(targeted_sentences,outli) and flag\n",
    "        if not flag:\n",
    "            break\n",
    "    # if type == \"a0 a1 B3 a3 a4\":\n",
    "    #     flag = get_mentioned_flag(targeted_sentences,outline[2])\n",
    "    # elif type == \"a0 B0 a2 B4 a4\":\n",
    "    #     flag1 = get_mentioned_flag(targeted_sentences,outline[1])\n",
    "    #     flag2 = get_mentioned_flag(targeted_sentences,outline[3])\n",
    "    #     flag = flag1 and flag2\n",
    "    # elif type == \"a0 B0 C0 a2 B4 C4 a4\":\n",
    "    #     flag1 = get_mentioned_flag(targeted_sentences,outline[1])\n",
    "    #     flag2 = get_mentioned_flag(targeted_sentences,outline[2])\n",
    "    #     flag3 = get_mentioned_flag(targeted_sentences,outline[4])\n",
    "    #     flag4 = get_mentioned_flag(targeted_sentences,outline[5])\n",
    "    #     flag = flag1 and flag2 and flag3 and flag4\n",
    "    # elif type == \"a0 B0 a1 B1 a2 B2 a3 B3 a4 B4\":\n",
    "    #     flag1 = get_mentioned_flag(targeted_sentences,outline[1])\n",
    "    #     flag2 = get_mentioned_flag(targeted_sentences,outline[3])\n",
    "    #     flag3 = get_mentioned_flag(targeted_sentences,outline[5])\n",
    "    #     flag4 = get_mentioned_flag(targeted_sentences,outline[7])\n",
    "    #     flag5 = get_mentioned_flag(targeted_sentences,outline[9])\n",
    "\n",
    "    #     flag = flag1 and flag2 and flag3 and flag4 and flag5\n",
    "    return flag\n",
    "\n",
    "\n",
    "def get_first_metric(results,type):\n",
    "    mentioned_flag_list = []\n",
    "    for i in trange(len(results)):\n",
    "        outline = results[i]['outline']\n",
    "        response = results[i]['response']\n",
    "        flag = get_plot_mentioned(outline,response,type)\n",
    "        mentioned_flag_list.append(flag)\n",
    "    return mentioned_flag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # last_story_part = text.rsplit(\"\\n\\nStory:\", 1)[-1]\n",
    "# def unwarp_response(response):\n",
    "#     if \"\\n\\nStory:\" in response:\n",
    "#         return response.rsplit(\"\\n\\nStory:\",1)[1].strip(), True\n",
    "#     elif \"\\n\\nstory:\" in response:\n",
    "#         return response.split(\"\\n\\nstory:\")[1].strip(), True\n",
    "#     elif \"\\n\\n**Story:\" in response:\n",
    "#         return response.split(\"\\n\\n**Story:\")[1].strip(), True\n",
    "#     elif \"\\n\\n**story:\" in response:\n",
    "#         return response.split(\"\\n\\n**story:\")[1].strip(), True\n",
    "#     else:\n",
    "#         return response, False\n",
    "\n",
    "# last_story_part = text.rsplit(\"\\n\\nStory:\", 1)[-1]\n",
    "def unwarp_response(response):\n",
    "    if \"\\n\\nStory:\" in response:\n",
    "        return response.rsplit(\"\\n\\nStory:\",1)[-1].strip(), True\n",
    "    elif \"\\n\\nstory:\" in response:\n",
    "        return response.rsplit(\"\\n\\nstory:\",1)[-1].strip(), True\n",
    "    elif \"\\n\\n**Story:\" in response:\n",
    "        return response.rsplit(\"\\n\\n**Story:\",1)[-1].strip(), True\n",
    "    elif \"\\n\\n**story:\" in response:\n",
    "        return response.rsplit(\"\\n\\n**story:\",1)[-1].strip(), True\n",
    "    else:\n",
    "        return response, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主线双支线.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:12<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主线双支线\n",
      "4.420634920634921\n",
      "主线支线.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:12<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主线支线\n",
      "3.7559055118110236\n",
      "双主线.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129/129 [00:12<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "双主线\n",
      "2.3203125\n",
      "主线情节点.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129/129 [00:12<00:00, 10.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主线情节点\n",
      "2.7734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "type_match = {\n",
    "    \"主线情节点\": \"a0 a1 B3 a3 a4\",\n",
    "    \"主线支线\": \"a0 B0 a2 B4 a4\",\n",
    "    \"主线双支线\": \"a0 B0 C0 a2 B4 C4 a4\",\n",
    "    \"双主线\": \"a0 B0 a1 B1 a2 B2 a3 B3 a4 B4\"\n",
    "}\n",
    "task_result = \"GPT4/COT/temp_0_7\"\n",
    "for filename in os.listdir(f\"{task_result}_results\"):\n",
    "    file_path = os.path.join(f\"{task_result}_results\", filename)\n",
    "    print(filename)\n",
    "    # if filename != \"主线情节点.jsonl\":\n",
    "    #     continue\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        results = []\n",
    "        for line in file:\n",
    "            res = {}\n",
    "            data = json.loads(line.strip())\n",
    "            \n",
    "            aid = data.get('Aid_list')\n",
    "            bid = data.get('Bid_list')\n",
    "            cid = data.get('Cid_list')\n",
    "            outline = data.get('outline')\n",
    "            raw_response = data.get('responses')\n",
    "            if raw_response == \"Error\":\n",
    "                continue\n",
    "            try:\n",
    "                response,flag = unwarp_response(raw_response)\n",
    "            except:\n",
    "                print(outline)\n",
    "                print(raw_response)\n",
    "\n",
    "            if not flag:\n",
    "                print(raw_response)\n",
    "                print(\"response not found\")\n",
    "                # raise ValueError(\"response not found\")\n",
    "                response = \"\"\n",
    "                # break\n",
    "            # print(response)\n",
    "            # print(\"---------------------\")\n",
    "\n",
    "            res['aid'] = aid\n",
    "            res['bid'] = bid\n",
    "            res['cid'] = cid\n",
    "            res['outline'] = outline\n",
    "            res['response'] = response\n",
    "            results.append(res)\n",
    "        type_file = file_path.split('/')[-1].split('.')[0]\n",
    "        type = type_match[type_file]\n",
    "        # mentioned_flag_list = get_first_metric(results,type)\n",
    "        count_list = get_Sec_metric(results)\n",
    "        print(type_file)\n",
    "        # output the true ratio in first metric\n",
    "        # print(mentioned_flag_list.count(True)/len(mentioned_flag_list))\n",
    "        # output the average count in second metric\n",
    "        print(sum(count_list)/len(count_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sam saw his brother Barry eating a peanut butter and banana sandwich.', 'Sam thought it was the grossest thing Sam ever saw.', \"During Holly's senior year Holly was able to apply to graduate schools.\", 'After many weeks, Sam reluctantly agreed to take a bite.', 'Sam now eats peanut butter and banana sandwiches every week.']\n",
      "Sam had always considered himself a man of simple tastes, with food preferences that leaned towards the traditional. But one afternoon, as he walked into the kitchen, he saw his brother Barry munching on something that threatened to topple his world of culinary certainty. Barry was eating a peanut butter and banana sandwich.\n",
      "\n",
      "The sight of the gooey peanut butter mixing with the squishy banana, the way Barry savored each bite, sent a shudder down Sam's spine. He stared, wide-eyed and aghast, as Barry finished the sandwich with a sat\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(count_list)):\n",
    "    if count_list[i]==0:\n",
    "        print(results[i]['outline'])\n",
    "        print(results[i]['response'])\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Joe walked out of the office with his coworkers, excited to try the new restaurant that had just opened up down the street. He hoped it would be his new go-to lunch spot, a place to escape the monotony of the usual sandwich shop.\\n\\nMeanwhile, in a different part of town, Frank was preparing for another game as a pitcher for a MLB team. It was a familiar routine for him, one he had perfected over the years. In fact, this was his second stint with the team, having made a successful comeback after a brief hiatus.\\n\\nAcross town, Ruth was chatting with her neighbor, a devoutly religious woman who always had a kind word and a warm smile. They talked about their weekends, sharing stories and laughter, before eventually parting ways. When they finished, Ruth headed back home, feeling uplifted by their conversation.\\n\\nBack at the office, Joe\\'s coworkers were discussing lunch options. \"Hey, we could just grab some burgers at the food truck,\" one of them suggested. \"Or hit up the deli for some quick sandwiches,\" another chimed in. Joe politely declined, his eyes fixed on the new restaurant. He was determined to try something new, something that would impress.\\n\\nFinally, the group arrived at the restaurant, and Joe\\'s anticipation was palpable. As they were led to their table, Joe couldn\\'t help but feel a sense of excitement. This could be it, his new lunch spot, his haven from the ordinary. He took a deep breath, ready to indulge in a culinary adventure.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\" Plan:\\n\\nI. Introduction\\n- Introduce Joe and his coworkers going to a new restaurant for lunch\\n- Establish Joe's hopes for a new lunch spot\\n\\nII. Interlude - Frank's Story\\n- Introduce Frank, a pitcher for a MLB team, in a separate storyline\\n- Show Frank's career progression, highlighting his return to the MLB team\\n\\nIII. Interlude - Ruth's Story\\n- Introduce Ruth and her religious neighbor\\n- Depict Ruth and her neighbor's interaction, ending with Ruth going home\\n\\nIV. Back to Joe's Story\\n- Joe's coworkers suggest cheap lunch options, contrasting with the new restaurant\\n- Joe's desire for a new lunch spot is reaffirmed\\n\\nV. Conclusion\\n- Joe and his coworkers arrive at the new restaurant, and Joe looks forward to trying it out\\n\\nStory:\\n\\nStory:\\n\\nJoe walked out of the office with his coworkers, excited to try the new restaurant that had just opened up down the street. He hoped it would be his new go-to lunch spot, a place to escape the monotony of the usual sandwich shop.\\n\\nMeanwhile, in a different part of town, Frank was preparing for another game as a pitcher for a MLB team. It was a familiar routine for him, one he had perfected over the years. In fact, this was his second stint with the team, having made a successful comeback after a brief hiatus.\\n\\nAcross town, Ruth was chatting with her neighbor, a devoutly religious woman who always had a kind word and a warm smile. They talked about their weekends, sharing stories and laughter, before eventually parting ways. When they finished, Ruth headed back home, feeling uplifted by their conversation.\\n\\nBack at the office, Joe's coworkers were discussing lunch options. \"Hey, we could just grab some burgers at the food truck,\" one of them suggested. \"Or hit up the deli for some quick sandwiches,\" another chimed in. Joe politely declined, his eyes fixed on the new restaurant. He was determined to try something new, something that would impress.\\n\\nFinally, the group arrived at the restaurant, and Joe's anticipation was palpable. As they were led to their table, Joe couldn't help but feel a sense of excitement. This could be it, his new lunch spot, his haven from the ordinary. He took a deep breath, ready to indulge in a culinary adventure. \"\"\"\n",
    "last_story_part = text.rsplit(\"\\n\\nStory:\", 1)[-1]\n",
    "last_story_part\n",
    "text.rsplit(\"\\n\\nStory:\",1)[-1].strip()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
