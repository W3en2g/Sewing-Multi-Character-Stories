{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"\"\n",
    ")\n",
    "\n",
    "def get_eval_prompt(response,reference_sentence):\n",
    "    prompt = f\"\"\"Please assess the degree of integration between a specific subplot and the main narrative of the story using the following scale of 0 to 100. If you determine that the subplot is seamlessly integrated and indispensable to the main storyline, award a score of 100. Conversely, if it stands entirely on its own with minimal relevance to the main plot, rate it as 0.\n",
    "\n",
    "### Subplot\n",
    "{reference_sentence}\n",
    "\n",
    "### Complete Story\n",
    "{response}\n",
    "\n",
    "Evaluate based on how closely the subplot is intertwined with the main story, and just provide a deterministic score followed by a concise and brief explanation, with a blank line between the two.\n",
    "\n",
    "Score:\n",
    "\n",
    "Explanation:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def asking(response,reference_sentence):\n",
    "    prompt = get_eval_prompt(response,reference_sentence)\n",
    "    # print(prompt)\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            # {\"role\": \"system\",\"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        model=\"gpt-4\",\n",
    "        temperature=0.0, \n",
    "        # top_p=TopPValue,\n",
    "        # max_tokens = 1024,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "    # print(chat_completion.choices[0].message.content)\n",
    "    # return upwarp_eval(chat_completion.choices[0].message.content)\n",
    "    # res = \"Score: 100\\nExplanation: The subplot is seamlessly integrated with the main narrative, and it is indispensable to the main storyline.\"\n",
    "    # return upwarp_eval(res)\n",
    "\n",
    "\n",
    "def upwarp_eval(eval_result):\n",
    "    # score = eval_result.split(\"Explanation:\")[0].split(\"Score:\")[1]\n",
    "    # explanation = eval_result.split(\"Explanation:\")[1]\n",
    "    score = eval_result.split(\"\\n\\n\")[0]\n",
    "    explanation = eval_result.split(\"\\n\\n\")[1]\n",
    "    return int(score), explanation,eval_result\n",
    "\n",
    "\n",
    "def ask_score_by_type(outline,response,type):\n",
    "\n",
    "    if type == \"a0 a1 B3 a3 a4\":\n",
    "        subplot = outline[2]\n",
    "        # score,explanation,eval_result = asking(response,subplot)\n",
    "        eval_result = asking(response,subplot)\n",
    "        # return [score], [explanation], [eval_result]\n",
    "        return [eval_result]\n",
    "    \n",
    "    elif type == \"a0 B0 a2 B4 a4\":\n",
    "        subplot = outline[1]+\" \"+outline[3]\n",
    "        # score,explanation,eval_result = asking(response,subplot)\n",
    "        eval_result = asking(response,subplot)\n",
    "        # return [score],[explanation], [eval_result]\n",
    "        return [eval_result]\n",
    "    elif type == \"a0 B0 C0 a2 B4 C4 a4\":\n",
    "        subplot1 = outline[1]+\" \"+outline[4]\n",
    "        # score1,explanation1,eval_result1 = asking(response,subplot1)\n",
    "        eval_result1 = asking(response,subplot1)\n",
    "        subplot2 = outline[2]+\" \"+outline[5]\n",
    "        # score2,explanation2,eval_result2 = asking(response,subplot2)\n",
    "        eval_result2 = asking(response,subplot2)\n",
    "        # return [score1,score2],[explanation1,explanation2], [eval_result1,eval_result2]\n",
    "        return [eval_result1,eval_result2]\n",
    "    \n",
    "    elif type == \"a0 B0 a1 B1 a2 B2 a3 B3 a4 B4\":\n",
    "        subplot1 = outline[1]+\" \"+outline[3]+\" \"+outline[5]+\" \"+outline[7]+\" \"+outline[9]\n",
    "        # score,explanation,eval_result = asking(response,subplot1)\n",
    "        eval_result = asking(response,subplot1)\n",
    "        # return [score],[explanation], [eval_result]\n",
    "        return [eval_result]\n",
    "    else:\n",
    "        print(\"No such type\")\n",
    "        return None\n",
    "\n",
    "from tqdm import trange\n",
    "def get_gpt4_evaluation(results,type):\n",
    "    # scores = []\n",
    "    # score_box_list = []\n",
    "    # explanation_box_list = []\n",
    "    eval_result_box_list = []\n",
    "    for i in trange(len(results)):\n",
    "        outline = results[i]['outline']\n",
    "        response = results[i]['response']\n",
    "        # score_box,explanation_box,eval_result_box = ask_score_by_type(outline,response,type)\n",
    "        eval_result_box = ask_score_by_type(outline,response,type)\n",
    "        # explanation_box_list.append(explanation_box)\n",
    "        eval_result_box_list.append(eval_result_box)\n",
    "        # if len(score_box) == 1:\n",
    "        #     score = score_box[0]\n",
    "        # else:\n",
    "        #     score = sum(score_box)/len(score_box)\n",
    "        # scores.append(score)\n",
    "    # return scores,score_box_list,explanation_box_list,eval_result_box_list\n",
    "    return eval_result_box_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [17:19<00:00,  8.00s/it]\n",
      "100%|██████████| 130/130 [08:33<00:00,  3.95s/it]\n",
      "100%|██████████| 130/130 [08:23<00:00,  3.87s/it]\n",
      "100%|██████████| 130/130 [08:55<00:00,  4.12s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "type_match = {\n",
    "    \"主线情节点\": \"a0 a1 B3 a3 a4\",\n",
    "    \"主线支线\": \"a0 B0 a2 B4 a4\",\n",
    "    \"主线双支线\": \"a0 B0 C0 a2 B4 C4 a4\",\n",
    "    \"双主线\": \"a0 B0 a1 B1 a2 B2 a3 B3 a4 B4\"\n",
    "}\n",
    "task_result = \"qwen2-70b/IO/temp_0_7\"\n",
    "\n",
    "for filename in os.listdir(f\"{task_result}_results\"):\n",
    "    file_path = os.path.join(f\"{task_result}_results\", filename)\n",
    "    type_file = file_path.split('/')[-1].split('.')[0]\n",
    "    type = type_match[type_file]\n",
    "    # if type_file != \"主线情节点\":\n",
    "    #     continue\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        results = []\n",
    "        for line in file:\n",
    "            res = {}\n",
    "            data = json.loads(line.strip())\n",
    "            \n",
    "            aid = data.get('Aid_list')\n",
    "            bid = data.get('Bid_list')\n",
    "            cid = data.get('Cid_list')\n",
    "            outline = data.get('outline')\n",
    "            response = data.get('responses')\n",
    "\n",
    "            res['aid'] = aid\n",
    "            res['bid'] = bid\n",
    "            res['cid'] = cid\n",
    "            res['outline'] = outline\n",
    "            res['response'] = response\n",
    "            results.append(res)\n",
    "\n",
    "        # print(type)\n",
    "        # gpt4_scores,score_box_list,explanation_box_list,eval_result_box_list = get_gpt4_evaluation(results,type)\n",
    "        eval_result_box_list = get_gpt4_evaluation(results,type)\n",
    "\n",
    "        # print(sum(gpt4_scores)/len(gpt4_scores))\n",
    "        # print(gpt4_scores)\n",
    "        # if file not exists, create it\n",
    "        if not os.path.exists(f\"{task_result}_eval_results\"):\n",
    "            os.makedirs(f\"{task_result}_eval_results\")\n",
    "        with open(f\"{task_result}_eval_results/{type_file}_eval_results.json\", 'w', encoding='utf-8') as file:\n",
    "            for i in range(len(results)):\n",
    "                res = {}\n",
    "                res['aid'] = results[i]['aid']\n",
    "                res['bid'] = results[i]['bid']\n",
    "                res['cid'] = results[i]['cid']\n",
    "                res['outline'] = results[i]['outline']\n",
    "                res['response'] = results[i]['response']\n",
    "                # res['score'] = gpt4_scores[i]\n",
    "                # res['explanation'] = explanation_box_list[i]\n",
    "                res['eval_result'] = eval_result_box_list[i]\n",
    "                file.write(json.dumps(res, ensure_ascii=False) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
